---
title: "Collecting and Analyzing Web Data - First Project"
author: "Virginia, Gina, Amy"
output: html_document
date: "last updated: `r Sys.Date()`"
---


*In groups of around 3-5 people, fill out this sheet on current debates about scraping, its scientific use and doing research in R. When you are finished, upload your solutions on GitHub.*

\setcounter{tocdepth}{2}
\tableofcontents

## Introduction

### GitHub

Your GitHub account:


### Previous experience

Fill in this [short survey on your previous experience](https://forms.gle/BJtFLNVWirnDWzSZ8)


\pagebreak


## Legal regulations and hacktivism

### Legality of scraping


Describe what the following terms / laws / cases refer to in one sentence each:

terms of service: They constitute a contract between the user and the govern, so they regulate the use and acces to the service.

GDPR: General Data Protection Regulation is a data protection law which changes the way companies can use the personal data of their customers.

Computer Fraud and Abuse Act: prohibits accessing a computer without authorization

Sandvig v. Barr: A lawsuit filed in the US 2016 that challenges the CFAA that terms of service on websites prohibit investigation of research practices of companies online. 



### Terms of service

Something that is not allowed: 

Something that is confusing: FB is allowed to store, copy and share photos on Facebook with others, such as service providers that support the service of facebook --> what means: service providers that support our service" exactly

Something related to scraping: 

You may not access or collect data from our Products using automated means (...) or attempt to access data you do not have permission to access. 

(txt file --> User-agent: *, Disallow: / Facebook prohibits all automated scrapers)


When planning to scrape a website, you should always check its robots.txt first. Robots.txt is a file used by websites to let "bots" know if or how the site should be scrapped or crawled and indexed. You could access the file by adding "/robots.txt" by the end of the link to your target website. 

### robots.txt


webpage you checked: https://www.facebook.com/robots.txt


### Hacktivism

Find information about the following people.

- Aaron Swartz
- Alexandra Elbakyan
- Karrie Karahalios
- Murray Cox
- Paolo Cirio
- another data activist of your choosing


### Scientific debate

article: https://www.tandfonline.com/doi/full/10.1080/10584609.2018.1477506

author's position: 
- keep the tenuousness of our access to digital data firmly in mind
- Web scraping is much more flexible than APIs but also more work-intensive and possibly illegal in some cases

\pagebreak


## Scientific use of scraping

### Use of scraping

Number of results:

Search terms:

### An article that uses webscraped data


article:

How is web data relevant:

### Your ideas for scraping

page:

idea:


\pagebreak


## Course practicalities

### Coding Style


Which tutorial / style guide did you look at?

One thing you want to improve about your R code:

One thing you disagree with:

### Coding for others



Which coding / style decisions were the most contested within your group?



### Work environment

How did you structure your folder?



### Submit!
